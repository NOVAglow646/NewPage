<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Qixun Wang (王启迅)</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Qixun Wang (王启迅)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://novaglow646.github.io/QixunWang-Homepage.github.io/"><img src="wqx2.jpg" alt="alt text" width="120px" /></a>&nbsp;</td>
<td align="left"><p>B.S Student<br />
<a href="https://eecs.pku.edu.cn/">School of Electronic Engineering and Computer Science</a> <br />
<a href="http://www.pku.edu.cn/">Peking University</a> <br />
No.5 Yiheyuan Road, Haidian District, Beijing 100871, P.R.China <br /> 
Email: qixun.wang@pku.edu.cn <br />
<br />
</td></tr></table>
<h2>Bio</h2>
<p>Currently I am an undergraduate student in the School of Electronic Engineering and Computer Science, Peking University.
    I major in machine intelligence.<br /></p>
<p>My research interests are situated in machine learning theory. More precisely, I am interested in seeking for ways of improving 
  the generalization ability and the transferability of the deep models. Among the related topics, I am most familiar with Domain Generalization (a.k.a Out-of-Distribution
  Generalization, OOD) and Domain Adaptation.
Additionally, I am also interested in Adversarial Learning and I have been doing research on establishing the connection between Adversarial Training (AT) and 
  Domain Generalization (DG). </p>
<h2>Educations</h2>


<h4>
    B.S, <a href="https://www.pku.edu.cn/">Peking University (PKU)</a> [2019.9 ~ 2023.6] 

</h4>



<h2>Publications</h2>
Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors (NeurIPS 2022 Accepted). 
<h2>Research Experiences</h2>
<ul>
    <li><p> Solving Domain Generalization via Adversarial Training with Structured Priors. <a href="OODPreprint.pdf"> [pdf] </a> 
    with Yifei Wang, Hong Zhu, advised by <a href="https://yisenwang.github.io/">Yisen Wang.</a> </p>
    <p>We investigate the limitness of OOD performance improvement of the conventional sample-wise Adversarial Training (AT) and empirically validate that the 
      low-rank structure that lies in the perturbation of the Universal Adversarial Training (UAT) is beneficial to OOD improvement. We further propose two low-rank 
      structured AT algorithms to alleviate this issue.
    </p>
    </li>
</ul>



<div id="footer">
<div id="footer-text">
<br>Page generated 2022-05-28, by <a href="https://novaglow646.github.io/QixunWang-Homepage.github.io/">Qixun Wang</a>.
</div>
</div>
</div>
</body>
</html>
